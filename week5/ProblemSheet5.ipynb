{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Sheet 5\n",
    "- To be completed by **12noon** on **Wednesday 6th November** and uploaded to [Problem Sheet 5 submission point](https://moodle.bath.ac.uk/mod/assign/view.php?id=1385492) on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (a): VC-dimension of the halfspaces classifier (lower bound)\n",
    "Consider the halfspaces prediction rule in the homogeneous form,\n",
    "$$\n",
    "h_{\\boldsymbol\\theta}(\\mathbf{x}) = \\mathrm{sign}(\\langle \\boldsymbol\\theta, \\mathbf{x}\\rangle) \\in \\mathcal{H}_n^{hs},\n",
    "$$\n",
    "where $\\mathbf{x} = \\begin{bmatrix}1\\\\ \\mathbf{\\hat x}\\end{bmatrix}$, $\\mathbf{\\hat x} \\in \\mathbb{R}^n$, $\\boldsymbol\\theta \\in \\mathbb{R}^{n+1}.$\n",
    "Consider a dataset \n",
    "$$\n",
    "\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_1 & \\cdots & \\mathbf{x}_{n+1} \\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & \\cdots & 1 & 1 \\\\\n",
    "0 & 1 & 0 & \\cdots & 0 & 0 \\\\\n",
    "0 & 0 & 1 & \\cdots & 0 & 0 \\\\\n",
    "\\vdots & & & \\ddots & & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & 1 & 0 \\\\\n",
    "0 & 0 & 0 & \\cdots & 0 & 1\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{(n+1) \\times (n+1)}\n",
    "$$\n",
    "- Show that for any $y_1,\\ldots,y_{n+1} \\in \\{-1,1\\}$ you can find $\\boldsymbol\\theta$ such that $h_{\\boldsymbol\\theta}(\\mathbf{x}_i) = y_i$, $i=1,\\ldots,n+1$.\n",
    "\n",
    "_Hint: you can solve the system of linear equations $\\langle \\boldsymbol\\theta, \\mathbf{x}_i\\rangle = y_i$ exactly, so the $\\mathrm{sign}()$ function is not needed in this case._\n",
    "\n",
    "**Remark:** therefore, $\\text{VC-dim}(\\mathcal{H}_n^{hs}) \\ge n+1$. In fact, one can prove that $\\text{VC-dim}(\\mathcal{H}_n^{hs}) = n+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification of Oxford temperature\n",
    "\n",
    "### Warm-up\n",
    "\n",
    "Here we want to predict whether the temperature in Oxford will be above or below 15 degrees Celsius in a given month. We choose a halfspaces prediction rule with polynomial features:\n",
    "$$\n",
    "h_{\\boldsymbol\\theta}(x) = \\mathrm{sign}\\left(\\theta_0 + \\theta_1 x + \\cdots + \\theta_n x^n - 15\\right)\n",
    "$$\n",
    "where $x$ is the month, and $\\boldsymbol\\theta = (\\theta_0,\\ldots,\\theta_n)$ is the vector of coefficients we will optimise. \n",
    "\n",
    "Here is the code as before reading the file `OxfordTemp.txt` for vectors of months `X` and temperatures `T`.\n",
    "\n",
    "**Note that the temperatures are now stored in a variable `T`$=(t_1,\\ldots,t_m)$, since $y_i$ will be the label $1$ if the temperature $t_i$ is above 15 degrees, and $-1$ otherwise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = np.loadtxt('OxfordTemp.txt')\n",
    "X = D[:,0]\n",
    "T = D[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the rest of the code from the previous problem sheets. For simplicity, we will optimise the same loss function as before,\n",
    "$$\n",
    "L_{D}(\\boldsymbol\\theta) = \\frac{1}{m}\\sum_{i=1}^{m} (\\theta_0 + \\theta_1 x_i + \\cdots + \\theta_n x_i^n - t_i)^2,\n",
    "$$\n",
    "even though $\\boldsymbol\\theta^* = \\arg\\min L_{D}(\\boldsymbol\\theta)$ may not necessarily be the best **binary** classification parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(x,n):\n",
    "    powers = np.arange(n+1)               # [0,1,2,...,n]\n",
    "    powers = np.reshape(powers, (1, -1))  # Make it explicitly a row vector\n",
    "    x = np.reshape(x, (-1, 1))            # Make it explicitly a column vector\n",
    "    return x**powers                      # Python automatically broadcasts the vectors to each others' shapes \n",
    "                                          # and takes the power between the resulting matrices elementwise\n",
    "\n",
    "def optimise_loss(V,y):\n",
    "    return np.linalg.solve(V.T @ V, V.T @ y)\n",
    "\n",
    "def split_data(X,Y,K,k):\n",
    "    # Create an index array, split that, and then take X and Y at this index array\n",
    "    N = X.shape[0]\n",
    "    test_range = range(k*N//K, (k+1)*N//K)  # index array of the test range\n",
    "    Xtest = X[test_range]\n",
    "    Xtrain = np.delete(X, test_range, axis=0)  # delete Xtest from X\n",
    "    Ytest = Y[test_range]\n",
    "    Ytrain = np.delete(Y, test_range, axis=0)  # delete Ytest from Y\n",
    "    return Xtrain, Ytrain, Xtest, Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "- **Modify** the $K$-fold Cross Validation function below to train $\\boldsymbol\\theta^{(k)}$ still using the vector of temperatures and the sum-of-squares loss as above, **but** now the test losses counting the **number of wrong binary** labels:\n",
    "  - $L_k:=\\sum_{(x,t) \\in D_{test}^{(k)}} |h_{\\boldsymbol\\theta^{(k)}}(x) - y(t)|/2$, where $y(t) = \\mathrm{sign}(t-15)$.\n",
    "  - Return $L_{cv} = \\frac{1}{K} \\sum_{k=0}^{K-1}L_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(X,Y,K,n):\n",
    "    ind = np.arange(X.size)\n",
    "    np.random.shuffle(ind)\n",
    "    L = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        Xtrain,Ytrain,Xtest,Ytest = split_data(X[ind],Y[ind],K,k)\n",
    "        V = features(Xtrain, n)\n",
    "        theta_k = optimise_loss(V, Ytrain)\n",
    "        V = features(Xtest, n)\n",
    "        L[k] = np.mean((V @ theta_k - Ytest)**2)\n",
    "    return np.mean(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "\n",
    "- **Vary** $n$ from 1 to 10, and for each $n$ compute and **plot** the $4$-fold cross validation loss as a function of $n$.\n",
    "- **Which** $n$ gives the smallest cross validation loss? How many wrong predictions you get on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
