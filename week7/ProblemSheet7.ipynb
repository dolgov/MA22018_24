{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc5921b0",
   "metadata": {},
   "source": [
    "# Problem Sheet 7\n",
    "\n",
    "- To be completed by **12noon** on **Wed 20th Nov** and uploaded to [Problem Sheet 7 submission point](https://moodle.bath.ac.uk/mod/assign/view.php?id=1389537) on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c1705",
   "metadata": {},
   "source": [
    "## Task (a) (Warm-up)\n",
    "\n",
    "Derive the gradients of:\n",
    "- $L(\\boldsymbol\\theta) = \\langle \\boldsymbol\\theta, \\mathbf{x} \\rangle$ for a fixed $\\mathbf{x}\\in\\mathbb{R}^n$;\n",
    "- $L(\\boldsymbol\\theta) = \\|\\boldsymbol\\theta\\|_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf4fb9",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "The typical approach is to write vector and matrix products elementwise, and compute a partial derivative with respect to one (but arbitrary) component $\\theta_j.$ \n",
    "\n",
    "- For the first function, we have:\n",
    "\\begin{align*}\n",
    "L(\\boldsymbol\\theta) &= \\langle \\boldsymbol\\theta, \\mathbf{x} \\rangle = \\sum_{i=1}^n \\theta_i x_i, & \\quad \\text{so} \\\\\n",
    "\\frac{\\partial L}{\\partial \\theta_j} &=  \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta_j} (\\theta_i x_i) = \\sum_{i=1}^n \\delta_{i,j} x_i, & \\quad \\text{where} \\\\\n",
    "\\delta_{i,j} & = \\begin{cases} 1 & \\text{if } i=j \\\\ 0 & \\text{otherwise,} \\end{cases} \n",
    "\\end{align*}\n",
    "since $\\theta_i$ is independent of $\\theta_j$.\n",
    "Thus, the sum over $i$ collapses to just \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta_j} = x_j \\quad \\Rightarrow \\quad \\nabla L = \\mathbf{x}.\n",
    "$$\n",
    "\n",
    "- For the second function, \n",
    "\\begin{align*}\n",
    "L(\\boldsymbol\\theta) &= \\|\\boldsymbol\\theta\\|_2^2 = \\sum_{i=1}^n \\theta_i^2, & \\quad \\text{so} \\\\\n",
    "\\frac{\\partial L}{\\partial \\theta_j} &=  \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta_j} (\\theta_i^2) = \\sum_{i=1}^n 2 \\delta_{i,j} \\theta_i, & \\text{which gives} \\\\\n",
    "\\frac{\\partial L}{\\partial \\theta_j} &= 2 \\theta_j \\quad \\Rightarrow \\quad \\nabla L = 2\\boldsymbol\\theta.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c8503",
   "metadata": {},
   "source": [
    "## Task (b)\n",
    "Consider a dataset $\\mathbf{D} = \\{(\\mathbf{x}_1,y_1), \\ldots, (\\mathbf{x}_m,y_m)\\}$ with $\\mathbf{x}_i \\in \\mathbb{R}^n$, $y_i \\in \\{-1,1\\}$, $i=1,\\ldots,m$.\n",
    "- Prove that the gradient of the minus-log-likelihood of the logistic regression of this dataset writes:\n",
    "$$\n",
    "\\nabla L_{\\mathbf{D}}(\\boldsymbol\\theta) = \\sum_{i=1}^m \\frac{-y_i \\mathbf{x}_i}{1 + \\exp(y_i \\langle \\boldsymbol\\theta, \\mathbf{x}_i \\rangle)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd6aef5",
   "metadata": {},
   "source": [
    "## Task (c)\n",
    "\n",
    "Consider a dataset $\\mathbf{D} = \\{(\\mathbf{x}_1,y_1), \\ldots, (\\mathbf{x}_m,y_m)\\}$ with $\\mathbf{x}_i \\in \\mathbb{R}^n$, $y_i \\in \\{-1,1\\}$, $i=1,\\ldots,m$,\n",
    "and assume that the derivative of $\\ell(t):=\\max\\{t,0\\}$ is extended to $\\ell'(t) = \\begin{cases}1, & t \\ge 0, \\\\ 0, & t<0. \\end{cases}$\n",
    "- Derive the gradient of the Soft SVM empirical risk,\n",
    "$$\n",
    "L_{\\mathbf{D}}(\\boldsymbol\\theta) = \\lambda \\|\\boldsymbol\\theta\\|_2^2 + \\frac{1}{m} \\sum_{i=1}^m \\max\\{1-y_i \\langle \\boldsymbol\\theta, \\mathbf{x}_i \\rangle, 0\\}.\n",
    "$$\n",
    "- How does the Gradient Descent iteration optimising the Soft SVM empirical risk compare to the Perceptron iteration?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7696aef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6ff76",
   "metadata": {},
   "source": [
    "## Task 1: logistic regression\n",
    "\n",
    "Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb06a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.vstack([np.linspace(-3,3,9).reshape(-1,1) + np.array([0, 1]),\n",
    "               np.linspace(-3,3,9).reshape(-1,1) + np.array([0,-1])])\n",
    "y = np.hstack((np.ones(9), -np.ones(9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66060a93",
   "metadata": {},
   "source": [
    "- Write a Python function `L(theta,X,y)` that takes as input a parameter vector `theta`=$\\boldsymbol\\theta\\in\\mathbb{R}^n$, a matrix `X`$\\in\\mathbb{R}^{m \\times n}$ of row vectors $\\mathbf{x}_1,\\ldots,\\mathbf{x}_m \\in \\mathbb{R}^n$ stacked vertically, and a vector of labels `y`=$(y_1,\\ldots,y_m)$, and returns the minus-log-likelihood $L_{\\mathbf{D}}(\\boldsymbol\\theta)$ of the logistic regression of the dataset $\\mathbf{D}$=(`X`,`y`) at the parameter `theta`.\n",
    "- Write a Python function `gL(theta,X,y)` that takes the same inputs as `L(theta,X,y)`, and returns the gradient vector $\\nabla L_{\\mathbf{D}}(\\boldsymbol\\theta)$ derived in Task (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14dae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d6a5141",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "- Write a Python code using [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) function (introduced in the previous problem sheet) to minimize $L_{\\mathbf{D}}$ from Task 1, starting from a zero initial guess.\n",
    "- Write a Python code to plot $\\mathbf{x}_1,\\ldots,\\mathbf{x}_m$ from Task 1 as points in $\\mathbb{R}^2$, and the separating line corresponding to $\\boldsymbol\\theta^*$ produced by scipy.optimize.minimize on the same plot. You may want to restrict `xlim` and `ylim` of the plot to $[-4,4]$ for convenient picture.\n",
    "- How accurate is the separating line visually, and how small is $\\|\\nabla L_{\\mathbf{D}}(\\boldsymbol\\theta^*)\\|_2$?\n",
    "\n",
    "_Hint: the separating line equation reads_ $\\langle \\boldsymbol\\theta^*, \\mathbf{x}\\rangle = 0$, _so the points on the line can be calculated as_ $\\mathbf{x} = t \\left[\\theta_2^*, -\\theta_1^*\\right]$ _for_ $t\\in\\mathbb{R},$ _which you can restrict to_ $t \\in [-1,1]$ _for plotting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0dbb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79a7d86a",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "- Copy over the Gradient Descent function `gd` from the `GD.ipynb` demonstration notebook here.\n",
    "- Modify the `gd` function to plot the values of the loss function $L_{\\mathbf{D}}(\\boldsymbol\\theta^k)$ at each iteration of the algorithm.\n",
    "- Repeat computations of Task 2 but replacing `scipy.optimize.minimize` with `gd` to find $\\boldsymbol\\theta^*$.\n",
    "- What happens? How accurate is the separating line visually, and how small is $\\|\\nabla L_{\\mathbf{D}}(\\boldsymbol\\theta^*)\\|_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb7e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
