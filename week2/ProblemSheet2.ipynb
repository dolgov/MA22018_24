{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Sheet 2\n",
    "\n",
    "- To be completed by **12noon** on Wed **16th Oct** and uploaded to [Problem Sheet 2 submission point](https://moodle.bath.ac.uk/mod/assign/view.php?id=1371305) on Moodle.\n",
    "\n",
    "In Tasks (a) and (b) we consider a simplified case of $n=1$ and $h_{\\theta}(x) = \\theta x$ and assume a quadratic pointwise loss, $\\ell(y,\\hat y) = (y-\\hat y)^2.$\n",
    "We also assume that data are samples of a random variable pair $(X,Y) \\sim \\mathbb{P}$.\n",
    "\n",
    "## Task (a) (Warm-up)\n",
    "\n",
    "- Find the expected risk $\\mathbb{E}[\\ell(h_{\\theta}(X),Y)]$ in terms of $\\theta$ and suitable expectations.\n",
    "- Find the best parameter $\\theta^{best} = \\arg\\min_{\\theta \\in \\mathbb{R}} \\mathbb{E}[\\ell(h_{\\theta}(X),Y)].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "$$\n",
    "\\mathbb{E}[\\ell(h_{\\theta}(X),Y)] = \\mathbb{E}[(\\theta X - Y)^2] = \\mathbb{E}[\\theta^2 X^2 - 2 \\theta X Y + Y^2] = \\theta^2 \\mathbb{E}[X^2] - 2 \\theta \\mathbb{E}[XY] + \\mathbb{E}[Y^2].\n",
    "$$\n",
    "Note that $\\theta$ is independent of $X$ and $Y$, hence it can be taken out of the expectation. In contrast, $X$ and $Y$ are dependent, so the product $XY$ should remain under the expectation.\n",
    "\n",
    "Differentiating this over $\\theta$ and taking the derivative to zero gives\n",
    "$$\n",
    "\\frac{d \\mathbb{E}[\\ell(h_{\\theta}(X),Y)]}{d\\theta}(\\theta^{best}) = 2 \\theta^{best} \\mathbb{E}[X^2] - 2  \\mathbb{E}[XY] = 0\n",
    "$$\n",
    "which gives\n",
    "$$\n",
    "\\theta^{best} = \\frac{\\mathbb{E}[XY]}{\\mathbb{E}[X^2]}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (b)\n",
    "- Assume $X$ is uniformly distributed on $[-1,1]$, $X \\sim \\mathcal{U}(-1,1)$, and consider two options of generating data:\n",
    "  - $Y = X$, and \n",
    "  - $Y = X - X^3$.\n",
    "  \n",
    "For each of those options for $Y$, compute $\\theta^{best}$ and the corresponding expected risk $\\mathbb{E}[\\ell(h_{\\theta^{best}}(X),Y)]$. What can you say about these $\\theta^{best}$ and the accuracy of the prediction rule $h_{\\theta^{best}}$ in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: simulating the \"true\" distribution\n",
    "\n",
    "For testing the convergence of the empirical risk we cannot use the temperature data (or any actual data in fact), since it's obviously finite, and we cannot let $|D| \\rightarrow \\infty$. To select a _class_ of models, or to test numerical algorithms, one often designs a **synthetic** distribution with known properties. One can then sample as many realisations from this distribution as desired.\n",
    "\n",
    "- **Write** a Python **function** `TrueSample()` which returns a pair x,y which is a random sample from the following distribution:\n",
    "   - $X \\sim \\mathcal{U}(-1,1)$ is a random value uniformly distributed between -1 and 1;\n",
    "   - for a given $X$, compute the label as $Y = X - X^3 + \\frac{1}{10} \\Xi$, where $\\Xi \\sim \\mathcal{N}(0,1)$ is a random variable with the standard normal distribution.   \n",
    "   \n",
    "   _Hint: read upon [numpy.random](https://numpy.org/doc/1.16/reference/routines.random.html) module_\n",
    " \n",
    "- **Compute** 30 samples of x and y from `TrueSample()` and store them in _numpy_ arrays X and Y, respectively.\n",
    "- **Plot** array Y vs array X using `plot()` from _matplotlib_'s _pyplot_ module. Make sure you plot **only points** (x,y) (not lines connecting them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt    \n",
    "\n",
    "###### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: convergence of the parameter\n",
    "- **Copy** over functions `features` and `optimise_loss` from Problem Sheet 1 (your implementation or my model solution)\n",
    "- Take $n=1$ and **compute** $\\boldsymbol\\theta^* \\in \\mathbb{R}^2$ using `optimise_loss` applied to the data X and Y prepared in Task 1.\n",
    "- **Vary** the number of samples in Task 1 in a range 30, 100, 300, 1000, 3000, and for each corresponding realisation of X and Y compute the corresponding $\\boldsymbol\\theta^*$.\n",
    "- **Compare** $\\theta^*_1$ with $\\theta^{best}$ found in Task (b) as the number of samples increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
