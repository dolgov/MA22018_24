{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a4a5fa",
   "metadata": {},
   "source": [
    "# Problem Sheet 4\n",
    "- To be completed by **12noon** on **Wednesday 30th October** and uploaded to [Problem Sheet 4 submission point](https://moodle.bath.ac.uk/mod/assign/view.php?id=1379919) on Moodle.\n",
    "\n",
    "## Clustering of text documents\n",
    "\n",
    "In this problem sheet we consider clustering algorithms and their application to text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb8c2a",
   "metadata": {},
   "source": [
    "## Task (a) (Warm-up): centroid with respect to Euclidean distance\n",
    "\n",
    "Let $C$ be a set of vectors $\\mathbf{x} \\in \\mathbb{R}^n$. \n",
    "\n",
    "- **Prove** that the vector\n",
    "$$\n",
    "\\boldsymbol\\mu = \\frac{1}{|C|} \\sum_{\\mathbf{x} \\in C} \\mathbf{x}\n",
    "$$\n",
    "(where $|C|$ is the cardinality of $C$) minimises the so-called _centroid_ loss $L(\\boldsymbol\\mu) := \\sum_{\\mathbf{x} \\in C} \\|\\mathbf{x}-\\boldsymbol\\mu\\|_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628a274",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "Writing $L$ componentwise, assuming $\\mathbf{x}=(x_1,\\ldots,x_n)$ and $\\boldsymbol\\mu=(\\mu_1,\\ldots,\\mu_n)$,\n",
    "$$\n",
    "L(\\boldsymbol\\mu) = \\sum_{\\mathbf{x}\\in C} \\|\\mathbf{x} - \\boldsymbol\\mu\\|_2^2 = \\sum_{\\mathbf{x}\\in C} \\sum_{j=1}^n (x_j - \\mu_j)^2 = \\sum_{j=1}^n \\underbrace{\\sum_{\\mathbf{x}\\in C} (x_j - \\mu_j)^2}_{L_j(\\mu_j)},\n",
    "$$\n",
    "we can minimise each component $L_j(\\mu_j)$ independently by taking its derivative to zero,\n",
    "$$\n",
    "\\frac{d L_j}{d \\mu_j} = \\sum_{\\mathbf{x}\\in C} \\left[2 (x_j - \\mu_j)\\right] = 2 \\sum_{\\mathbf{x}\\in C} x_j - 2\\sum_{\\mathbf{x}\\in C} \\mu_j = 2 \\sum_{\\mathbf{x}\\in C} x_j - 2 |C| \\mu_j = 0.\n",
    "$$\n",
    "Solving this we get $\\mu_{j} = \\frac{1}{|C|} \\sum_{\\mathbf{x}\\in C} x_j$ for the elements of the centroid $\\boldsymbol\\mu = (\\mu_{1},\\ldots,\\mu_{n})$, as requested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d14bda",
   "metadata": {},
   "source": [
    "## Task (b): K-means with cosine similarity score\n",
    "\n",
    "- **Prove** that if $\\|\\mathbf{x}\\|_2=\\|\\boldsymbol\\mu\\|_2=1$ $\\forall \\mathbf{x},\\boldsymbol\\mu \\in \\mathbb{R}^n$, then \n",
    "$$\n",
    "\\|\\mathbf{x} - \\boldsymbol\\mu\\|_2^2 = 2 - 2\\cos\\angle(\\mathbf{x},\\boldsymbol\\mu).\n",
    "$$\n",
    "\n",
    "_Hint: expand $\\|\\mathbf{x} - \\boldsymbol\\mu\\|_2^2$ using inner products of vectors._\n",
    "\n",
    "**Remark**: therefore, lower Euclidean distance is equivalent to higher cosine score, and the standard K-means algorithm with Euclidean distance can be used if term-to-document vectors are produced with `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b05ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717fd30",
   "metadata": {},
   "source": [
    "## Task 1 (Warm-up)\n",
    "**Write** (or borrow) a Python **code** to apply the K-means algorithm (with $K=2$) with the Euclidean distance function to the following term-to-document matrix:\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "     1 & 1 & 0 & 0 & 1 \\\\\n",
    "     0 & 1 & 1 & 0 & 1 \\\\\n",
    "     0 & 0 & 0 & 1 & 1\n",
    "    \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1beb7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "X = np.array([[1,1,0,0,1], [0,1,1,0,1], [0,0,0,1,1]])\n",
    "kmeans = KMeans(n_clusters=2, n_init='auto')\n",
    "kvec = kmeans.fit_predict(X)\n",
    "kvec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58c5c9",
   "metadata": {},
   "source": [
    "## Task 2: Information retrieval of Bath Pages\n",
    "\n",
    "Write a Python code which:\n",
    "- Loads the Bath website text data from `BathPages.npz` (if you don't see it in the `week4` folder on Noteable, remember to run the `Update` notebook).\n",
    "- Computes its TF $\\cdot$ IDF term-to-document matrix with the 2-norm of each row equal 1 as required in Task (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4152c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38046651",
   "metadata": {},
   "source": [
    "## Task 3: K-means clustering of Bath pages\n",
    "\n",
    "- **Write** a Python **code** that:\n",
    "  - varies the number of clusters $K$ from 2 to 20 (inclusive);\n",
    "  - for each $K$ runs the $K$-means algorithm 10 times to clusterise the rows of the TF $\\cdot$ IDF term-to-document matrix from Task 2 and compute the $K$-means loss of that clustering;\n",
    "  - once both loops are complete, plots the $K$-means loss as a function of $K$ for each of the 10 runs on the same plot. Keep in mind that $K$ is a discrete variable.\n",
    "\n",
    "- **Find** an elbow point on the $K$-means loss plot. Use your own judgement to decide which number of clusters $K^*$ is a reasonable elbow point.\n",
    "- **Print** the titles of Bath Pages in each of the $K^*$ clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d45d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
