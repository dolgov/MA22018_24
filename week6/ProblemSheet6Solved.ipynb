{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Sheet 6\n",
    "\n",
    "- To be completed by **12noon** on **Wed 13th Nov** and uploaded to [Problem Sheet 6 submission point](https://moodle.bath.ac.uk/mod/assign/view.php?id=1388402) on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (a)\n",
    "\n",
    "Prove that Equations (4.6) and (4.7) in the typed notes are equivalent,\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{\\substack{\\boldsymbol\\theta\\in\\mathbb{R}^{n+1}\\\\ \\|\\boldsymbol\\theta\\|_2=1 \\\\ y_i \\langle \\boldsymbol\\theta, \\mathbf{x}_i\\rangle > 0}} \\min_{i=1,\\ldots,m} |\\langle \\boldsymbol\\theta, \\mathbf{x}_i\\rangle| & = \n",
    "\\max_{\\substack{\\boldsymbol\\theta\\in\\mathbb{R}^{n+1}\\\\ \\|\\boldsymbol\\theta\\|_2=1}} \\min_{i=1,\\ldots,m} y_i \\langle \\boldsymbol\\theta, \\mathbf{x}_i\\rangle. \n",
    "\\end{align*}\n",
    "\n",
    "_Hint: note that since $\\mathbf{D}$ is separable, there exists at least one $\\boldsymbol\\theta^*$ such that $y_i \\langle \\boldsymbol\\theta^*, \\mathbf{x}_i\\rangle>0,$ so both minimum and maximum of $y_i \\langle \\boldsymbol\\theta, \\mathbf{x}_i\\rangle$ have to be positive, and consider separately the cases $y_i=1$ and $y_i=-1$._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "Since there exists at least one feasible solution with $y_i \\langle \\boldsymbol\\theta^*, \\mathbf{x}_i\\rangle>0,$ Equations (4.6) and (4.7) can be considered only for those $\\boldsymbol\\theta$ where $y_{i_*} \\langle \\boldsymbol\\theta, \\mathbf{x}_{i_*}\\rangle>0,$\n",
    "where $i_* = \\arg\\min_{i=1,\\ldots,m} y_{i} \\langle \\boldsymbol\\theta, \\mathbf{x}_{i}\\rangle>0.$\n",
    "Now,\n",
    "\n",
    "- if $y_{i_*}=-1$, it must be $\\langle \\boldsymbol\\theta, \\mathbf{x}_{i_*}\\rangle < 0$, and hence $y_{i_*} \\langle \\boldsymbol\\theta, \\mathbf{x}_{i_*}\\rangle = - \\langle \\boldsymbol\\theta, \\mathbf{x}_{i_*}\\rangle = |\\langle \\boldsymbol\\theta, \\mathbf{x}_{i_*}\\rangle|$.\n",
    "- If $y_{i_*}=1$, then $0<\\langle \\boldsymbol\\theta, \\mathbf{x}_{i_*}\\rangle = |\\langle \\boldsymbol\\theta, \\mathbf{x}_{i_*}\\rangle|$\n",
    "\n",
    "In both cases we get (4.6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (b) (Warm-up)\n",
    "\n",
    "Prove that \n",
    "$$\n",
    "\\min_{i=1,\\ldots,m} y_i \\left\\langle \\frac{\\tilde{\\boldsymbol\\theta}}{\\|\\tilde{\\boldsymbol\\theta}\\|_2}, \\mathbf{x}_i \\right\\rangle \\ge \\frac{1}{\\|\\boldsymbol\\eta\\|_2}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\tilde{\\boldsymbol\\theta} = \\arg\\min_{\\boldsymbol\\theta\\in\\mathbb{R}^{n+1}} \\|\\boldsymbol\\theta\\|_2^2 \\quad \\text{such that} \\quad y_i \\langle \\boldsymbol\\theta, \\mathbf{x}_i \\rangle \\ge 1 \\quad \\forall i,\n",
    "$$\n",
    "for any $\\boldsymbol\\eta \\in \\mathbb{R}^{n+1}$ such that $y_i \\langle \\boldsymbol\\eta, \\mathbf{x}_i \\rangle \\ge 1$ $\\forall i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "Since $y_i \\langle \\tilde{\\boldsymbol\\theta}, \\mathbf{x}_i \\rangle \\ge 1$ for any $i=1,\\ldots,m,$\n",
    "$$\n",
    "y_i \\left\\langle \\frac{\\tilde{\\boldsymbol\\theta}}{\\|\\tilde{\\boldsymbol\\theta}\\|_2}, \\mathbf{x}_i \\right\\rangle \\ge \\frac{1}{\\|\\tilde{\\boldsymbol\\theta}\\|_2}\n",
    "$$\n",
    "for any $i=1,\\ldots,m,$ including that realising the $\\min$.\n",
    "In turn, since $\\tilde{\\boldsymbol\\theta}$ is the minimiser of the norm, $\\|\\tilde{\\boldsymbol\\theta}\\|_2 \\le \\|\\boldsymbol\\eta\\|_2$ for any other $\\boldsymbol\\eta \\in \\mathbb{R}^{n+1}$ such that $y_i \\langle \\boldsymbol\\eta, \\mathbf{x}_i \\rangle \\ge 1.$\n",
    "Thus,\n",
    "$$\n",
    "y_i \\left\\langle \\frac{\\tilde{\\boldsymbol\\theta}}{\\|\\tilde{\\boldsymbol\\theta}\\|_2}, \\mathbf{x}_i \\right\\rangle \\ge \\frac{1}{\\|\\tilde{\\boldsymbol\\theta}\\|_2} \\ge \\frac{1}{\\|\\boldsymbol\\eta\\|_2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (c)\n",
    "\n",
    "Prove the rest of Theorem 4.11 in the lecture notes: that you can take $\\boldsymbol\\eta = \\frac{\\boldsymbol\\theta^*}{\\gamma}$ in Task (b),\n",
    "where \n",
    "$$\n",
    "\\boldsymbol\\theta^* = \\arg\\max_{\\substack{\\boldsymbol\\theta\\in\\mathbb{R}^{n+1}\\\\ \\|\\boldsymbol\\theta\\|_2=1}} \\min_{i=1,\\ldots,m} y_i \\langle \\boldsymbol\\theta, \\mathbf{x}_i\\rangle\n",
    "$$\n",
    "is the maximal-margin parameter, and \n",
    "$$\n",
    "\\gamma = \\min_{i=1,\\ldots,m} y_i \\langle \\boldsymbol\\theta^*, \\mathbf{x}_i\\rangle\n",
    "$$ \n",
    "is the corresponding maximal margin, after which Task (b) implies that $\\tilde{\\boldsymbol\\theta}/\\|\\tilde{\\boldsymbol\\theta}\\|_2$ is the maximal-margin parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "We need to check that $y_i \\langle \\boldsymbol\\eta, \\mathbf{x}_i \\rangle \\ge 1.$\n",
    "Indeed,\n",
    "$$\n",
    "y_i \\langle \\boldsymbol\\eta, \\mathbf{x}_i \\rangle = \\frac{1}{\\gamma} y_i \\langle \\boldsymbol\\theta^*, \\mathbf{x}_i \\rangle \\ge  y_{i_*} \\langle \\boldsymbol\\theta^*, \\mathbf{x}_{i_*} \\rangle = \\frac{\\gamma}{\\gamma} = 1.\n",
    "$$\n",
    "However, then from Task (b),\n",
    "$$\n",
    "y_i \\left\\langle \\frac{\\tilde{\\boldsymbol\\theta}}{\\|\\tilde{\\boldsymbol\\theta}\\|_2}, \\mathbf{x}_i \\right\\rangle \\ge \\frac{1}{\\|\\boldsymbol\\eta\\|_2} = \\frac{\\gamma}{\\|\\boldsymbol\\theta^*\\|_2} = \\gamma\n",
    "$$\n",
    "for any $i$ including that realising the $\\min$, but $\\gamma$ is already the maximal margin, so $\\frac{\\tilde{\\boldsymbol\\theta}}{\\|\\tilde{\\boldsymbol\\theta}\\|_2}$ is the maximiser of the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "- Copy over the code setting up the dataset in Example 4.8 from the Perceptron notebook.\n",
    "- Read about [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) function.\n",
    "- Write a Python code using this function to minimise the soft SVM loss from the lectures,\n",
    "$$\n",
    "L_{\\mathbf{D}}(\\boldsymbol\\theta) = \\lambda \\|\\boldsymbol\\theta\\|_2^2 + \\frac{1}{m} \\sum_{i=1}^m \\max\\{1 - y_i \\langle \\boldsymbol\\theta, \\mathbf{x}_i\\rangle, 0\\}.\n",
    "$$\n",
    "- Apply this code to classify the first 4 vectors from Example 4.8 and print the minimiser $\\boldsymbol\\theta^* = \\arg\\min L_{\\mathbf{D}}(\\boldsymbol\\theta)$ and the halfspaces predictions $\\text{sign}(\\langle \\boldsymbol\\theta^*, \\mathbf{x}_i\\rangle)$ for all $\\mathbf{x}_i$ in the dataset.\n",
    "- Are the predictions accurate? How does $\\boldsymbol\\theta^*$ compare to the optimal parameters in the Perceptron and `SVC` algorithms demonstrated in the lectures?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#               AND OFFER THE  OF SALE\n",
    "Xh = np.array([[ 1 ,  1 ,  0 , 1 , 1],   # x_1   spam\n",
    "               [ 0 ,  0 ,  1 , 1 , 0],   # x_2   not spam\n",
    "               [ 0 ,  1 ,  1 , 0 , 0],   # x_3   spam\n",
    "               [ 1 ,  0 ,  0 , 1 , 0],   # x_4   not spam\n",
    "               [ 1 ,  0 ,  1 , 0 , 1],   # x_5   spam\n",
    "               [ 1 ,  0 ,  1 , 1 , 0]    # x_6   not spam\n",
    "             ])\n",
    "y = np.array([1,-1,1,-1,1,-1])\n",
    "X = np.insert(Xh, 0, 1.0, axis=1) # Affine -> Homogeneous form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta*: [-1.73846871e-08 -1.73846871e-08  9.99999966e-01 -1.73846871e-08\n",
      " -5.00000000e-01  4.99999983e-01]\n",
      "predictions: [ 1. -1.  1. -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def svm_loss(theta, X, y, Lam=1):\n",
    "    return Lam*np.sum(theta**2) + np.mean(np.maximum(1 - y*(X @ theta), np.zeros_like(y)))\n",
    "\n",
    "sol = minimize(lambda theta: svm_loss(theta, X[:4], y[:4], 0.25), np.zeros(X.shape[1]))\n",
    "print('theta*:', sol.x)\n",
    "print(\"predictions:\", np.sign(X @ sol.x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
